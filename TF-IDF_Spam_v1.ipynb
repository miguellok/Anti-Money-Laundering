{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "filepath='spam.csv'\n",
    "def readData_rawSMS(filepath):\n",
    "    data_rawSMS = pd.read_csv(filepath,usecols=[0,1],encoding='latin-1')\n",
    "    data_rawSMS.columns=['label','content']\n",
    "    return data_rawSMS\n",
    "data_rawSMS = readData_rawSMS(filepath)\n",
    "#########################################\n",
    "for i in range(data_rawSMS.shape[0]):\n",
    "    if data_rawSMS.iloc[i].label == 'ham':\n",
    "        data_rawSMS.iloc[i].label='genuine'\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.將資料分成Train和Test\n",
    "def Separate_TrainAndTest(data_rawSMS):\n",
    "    n=int(data_rawSMS.shape[0])\n",
    "    tmp_train=(np.random.rand(n)>=0.5)\n",
    "    return data_rawSMS.iloc[np.where(tmp_train==True)[0]], data_rawSMS.iloc[np.where(tmp_train==False)[0]]\n",
    "data_rawtrain,data_rawtest=Separate_TrainAndTest(data_rawSMS)\n",
    "\n",
    "#3. 從training data去著手算哪些「詞」重要。\n",
    "import re\n",
    "def generate_key_list(data_rawtrain, size_table=200,ignore=3):\n",
    "    dict_spam_raw = dict()\n",
    "    dict_genuine_raw = dict()\n",
    "    dict_IDF = dict()\n",
    "    # ignore all other than letters.\n",
    "    for i in range(data_rawSMS.shape[0]):\n",
    "        finds = re.findall('[A-Za-z]+', data_rawSMS.iloc[i].content)\n",
    "        if data_rawSMS.iloc[i].label == 'spam':\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower() #英文轉成小寫\n",
    "                try:\n",
    "                    dict_spam_raw[find] = dict_spam_raw[find] + 1\n",
    "                except:\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,1)\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,0)\n",
    "        else:\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower()\n",
    "                try:\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw[find] + 1\n",
    "                except:\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,1)\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,0)\n",
    "        word_set = set()\n",
    "        for find in finds:\n",
    "            if len(find)<ignore: continue\n",
    "            find = find.lower()\n",
    "            if not(find in word_set):\n",
    "                try:\n",
    "                    dict_IDF[find] = dict_IDF[find] + 1\n",
    "                except:\n",
    "                    dict_IDF[find] = dict_IDF.get(find,1)\n",
    "            word_set.add(find)\n",
    "    word_df = pd.DataFrame(list(zip(dict_genuine_raw.keys(),dict_genuine_raw.values(),dict_spam_raw.values(),dict_IDF.values())))\n",
    "    word_df.columns = ['keyword','genuine','spam','IDF']\n",
    "    word_df['genuine'] = word_df['genuine'].astype('float')/data_rawtrain[data_rawtrain['label']=='genuine'].shape[0]\n",
    "    word_df['spam'] = word_df['spam'].astype('float')/data_rawtrain[data_rawtrain['label']=='spam'].shape[0]\n",
    "    word_df['IDF'] = np.log10(word_df.shape[0]/word_df['IDF'].astype('float'))\n",
    "    word_df['genuine_IDF'] = word_df['genuine']*word_df['IDF']\n",
    "    word_df['spam_IDF'] = word_df['spam']*word_df['IDF']\n",
    "    word_df['diff']=word_df['spam_IDF']-word_df['genuine_IDF']\n",
    "    selected_spam_key = word_df.sort_values('diff',ascending=False)  \n",
    "    keyword_dict = dict()\n",
    "    i = 0\n",
    "    for word in selected_spam_key.head(size_table).keyword:\n",
    "        keyword_dict.update({word.strip():i})\n",
    "        i+=1\n",
    "    return keyword_dict   \n",
    "# build a tabu list based on the training data\n",
    "size_table = 300                 # how many features are used to classify spam\n",
    "word_len_ignored = 3            # ignore those words shorter than this variable\n",
    "keyword_dict=generate_key_list(data_rawtrain, size_table, word_len_ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.將Train資料和Test資料轉換成特徵向量\n",
    "def convert_Content(content, keyword_dict):\n",
    "    m = len(keyword_dict)\n",
    "    res = np.int_(np.zeros(m))\n",
    "    finds = re.findall('[A-Za-z]+', content)\n",
    "    for find in finds:\n",
    "        find=find.lower()\n",
    "        try:\n",
    "            i = keyword_dict[find]\n",
    "            res[i]=1\n",
    "        except:\n",
    "            continue\n",
    "    return res\n",
    "def raw2feature(data_rawtrain,data_rawtest,keyword_dict):\n",
    "    n_train = data_rawtrain.shape[0]\n",
    "    n_test = data_rawtest.shape[0]\n",
    "    m = len(keyword_dict)\n",
    "    X_train = np.zeros((n_train,m));\n",
    "    X_test = np.zeros((n_test,m));\n",
    "    Y_train = np.int_(data_rawtrain.label=='spam')\n",
    "    Y_test = np.int_(data_rawtest.label=='spam')\n",
    "    for i in range(n_train):\n",
    "        X_train[i,:] = convert_Content(data_rawtrain.iloc[i].content, keyword_dict)\n",
    "    for i in range(n_test):\n",
    "        X_test[i,:] = convert_Content(data_rawtest.iloc[i].content, keyword_dict)\n",
    "        \n",
    "    return [X_train,Y_train],[X_test,Y_test]\n",
    "     \n",
    "Train,Test=raw2feature(data_rawtrain,data_rawtest,keyword_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuarcy NBclassifier : 97.80％\n",
      "Training Accuarcy RF: 99.65％\n"
     ]
    }
   ],
   "source": [
    "# 5.依據特徵資料訓練分類器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB       \n",
    "def learn(Train):\n",
    "    model_NB = BernoulliNB()\n",
    "    model_NB.fit(Train[0], Train[1])\n",
    "    Y_hat_NB = model_NB.predict(Train[0])\n",
    "\n",
    "    model_RF = RandomForestClassifier(n_estimators=10, max_depth=None,\\\n",
    "                                 min_samples_split=2, random_state=0)\n",
    "    model_RF.fit(Train[0], Train[1])\n",
    "    Y_hat_RF = model_RF.predict(Train[0])\n",
    "    \n",
    "    n=np.size(Train[1])\n",
    "    print('Training Accuarcy NBclassifier : {:.2f}％'.format(sum(np.int_(Y_hat_NB==Train[1]))*100./n))\n",
    "    print('Training Accuarcy RF: {:.2f}％'.format(sum(np.int_(Y_hat_RF==Train[1]))*100./n))\n",
    "    return model_NB,model_RF\n",
    "# train the Random Forest and the Naive Bayes Model using training data\n",
    "model_NB,model_RF=learn(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuarcy: 98.45％ (sklearn.naive_bayes)\n",
      "Testing Accuarcy: 97.27％ (sklearn.ensemble._forest)\n"
     ]
    }
   ],
   "source": [
    "# 6.依據訓練好的分類器，進行測試。\n",
    "def test(Test,model):\n",
    "    Y_hat = model.predict(Test[0])\n",
    "    n=np.size(Test[1])\n",
    "    print ('Testing Accuarcy: {:.2f}％ ({})'.format(sum(np.int_(Y_hat==Test[1]))*100./n,model.__module__))\n",
    "# Test Model using testing data\n",
    "test(Test,model_NB)\n",
    "test(Test,model_RF)\n",
    "\n",
    "#######\n",
    "def predictSMS(SMS,model,keyword_dict):\n",
    "    X = convert_Content(SMS, keyword_dict)\n",
    "    Y_hat = model.predict(X.reshape(1,-1))\n",
    "    if int(Y_hat) == 1:\n",
    "        print ('SPAM: {}'.format(SMS))\n",
    "    else:\n",
    "        print ('GENUINE: {}'.format(SMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAM: go to visit www.yahoo.com.tw, Buy one get one free, Hurry!\n",
      "GENUINE: Call back for anytime.\n"
     ]
    }
   ],
   "source": [
    "inputstr='go to visit www.yahoo.com.tw, Buy one get one free, Hurry!'\n",
    "predictSMS(inputstr,model_NB,keyword_dict)\n",
    "\n",
    "inputstr=('Call back for anytime.')\n",
    "predictSMS(inputstr,model_NB,keyword_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "352.4px",
    "left": "439.4px",
    "right": "20px",
    "top": "120px",
    "width": "391px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
